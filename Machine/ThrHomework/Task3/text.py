# 1. Study the basic [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) algorithm using the link or lecture notes.

# 2. Implement a simple gradient descent algorithm with a constant step to solve the problem of finding the minimum of some function of two variables. Graphically confirm your calculations (an example was shown in the lecture).

# 3. Using gradient descent, implement your own learning function for any of the regressions studied earlier. Test it thoroughly. Will your algorithm always converge (produce the correct solution)? What parameters does the algorithm's operation depend on? Compare the obtained results with built-in functions.
# 4. Think about improving the algorithm in terms of convergence.

"""
1. 使用链接或讲义学习基本的[梯度下降]（https://en.wikipedia.org/wiki/Gradient_descent）算法。



2. 实现一个简单的梯度下降算法，用一个常数步长来解决寻找两个变量函数的最小值的问题。图形化地确认你的计算（讲座中有一个例子）。



3. 使用梯度下降，实现你自己的学习函数的任何回归研究之前。彻底地测试它。你的算法是否总是收敛（产生正确的解）？算法的运行依赖于哪些参数？将获得的结果与内置函数进行比较。

4. 考虑一下在收敛性方面改进算法。

"""
