# CDF:ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$
å·²çŸ¥\ \begin{array}{c|lcr}
    y & \text{0} & \text{1} \\
    \hline
    P & \frac{1}{2} & \frac{1}{2}\\
\end{array},
\ n=1000,X=y_1+â€¦â€¦+y_{1000}
\\æ±‚E(X),D(X),\sigma(X)
\\è§£:å·²çŸ¥E(y)=\frac{1}{2}\\D(y)=E(y^2)-(E(y)^2)=\frac{1}{2} \cdot0^2+\frac{1}{2}\cdot1^2-(\frac{1}{2})^2=\frac{1}{4}\\
æ‰€ä»¥E(X)=1000\cdot\frac{1}{2}=500\\D(X)=1000\cdot\frac{1}{4}=250
\\\sigma(X)=\sqrt{D(X)}=\sqrt{250}\approx\sqrt{256}=16\\
ä¸”ä¸€èˆ¬æƒ…å†µä¸‹æ»¡è¶³æ ‡å‡†æ­£æ€åˆ†å¸ƒ\implies\frac{X-E(X)}{\sigma(X)}\backsim N(0,1)
\\åŒç†æ™®é€šæ­£æ€åˆ†å¸ƒä¸ºX \backsim N(\mu,\sigma^2),P(X)=\frac{1}{\sqrt{2\pi}\cdot\sigma}\cdot e^{-\frac{(x-\mu)^2}{2\cdot\sigma^2}}
\\è‹¥ä»¤Z=\frac{X-E(X)}{\sigma(X)}\implies Z \backsim N(0,1),P(Z)=\frac{1}{\sqrt{2\pi}}\cdot e^{-\frac{x^2}{2}}\\
é‚£ä¹ˆP(Z\leq0.7)=\int_0^{0.7}P(Z)=\int_0^{0.7}\frac{1}{\sqrt{2\pi}}\cdot e^{-\frac{x^2}{2}}\cdot dx 
\\è¿™æ ·è®¡ç®—è¿‡äºéº»çƒ¦ï¼Œäºæ˜¯å¼•å…¥äº†æ ‡å‡†æ­£æ€çš„ä¸€ä¸ªç´¯è®¡åˆ†å¸ƒå‡½æ•°CDF\\
\phi(a)=P(Z\leq a)
\\ \phi(a)=\int_{-\infty}^{a}\frac{1}{\sqrt{2\pi}}\cdot e^{-\frac{x^2}{2}}\cdot dx 
\\ \phi_0(a)=\int_{0}^{a}\frac{1}{\sqrt{2\pi}}\cdot e^{-\frac{x^2}{2}}\cdot dx 
\\
ä¾‹å¦‚ï¼šè¦æ±‚P(x>\xi)=P(\frac{x-500}{16}>\frac{\xi-500}{16})
\\P(x>500)=P(\frac{x-500}{16}>\frac{500-500}{16})=P(Z>0)=1-\phi(0)
\\P(x>530)=P(\frac{x-530}{16}>\frac{530-500}{16})=P(Z>\frac{30}{16})=1-\phi(\frac{30}{16})
\\P(x>550)=P(\frac{x-550}{16}>\frac{550-500}{16})=P(Z>\frac{50}{16})=1-\phi(\frac{50}{16})
$$

# CLT:ä¸­å¿ƒæé™å®šç†

$$
å½“nåŠå…¶å¤§çš„æ—¶å€™ï¼ŒE(X)=\muï¼ŒD(X)=\sigma^2\\
è‹¥Xæ”¶æ•›åˆ™æœ‰X_1+â€¦â€¦+X_n\backsim N(n\cdot\mu,n\cdot\sigma^2)\ or \ N(n\cdot\mu,(\sqrt{n}\cdot\sigma)^2)
\\ä¸‰\sigmaåŸåˆ™ï¼š\\è¿˜æ˜¯ä¸Šé¢çš„ä¾‹å­ \sigma=16ï¼Œn=1000\\
\\P(\mu-\sigma<x<\mu+\sigma)=P(500-16<x<500+16)\approx0.68
\\P(\mu-2\sigma<x<\mu+2\sigma)=P(468<x<532)\approx0.96
\\P(\mu-3\sigma<x<\mu+3\sigma)=P(453<x<548)\approx0.999

\\ \color{yellow}{ä¾‹é¢˜}\\
n=100æ±‚P(40\leq X\leq 60);n=1000æ±‚P(400\leq X\leq 600)
\\E(X)=\frac{1}{2},D(X)=\frac{1}{4}
\\E(X_{100})=\frac{1}{2}\cdot100=50,D(X_{100})=\frac{1}{4}\cdot100=25,\sigma(X_{100})=5
\\E(X_{1000})=\frac{1}{2}\cdot1000=500,D(X_{1000})=\frac{1}{4}\cdot1000=250,\sigma(X_{1000})=16
\\Z=\frac{X-E(X)}{\sigma(X)}
\\P(40\leq X\leq 60)=P(\frac{40-50}{5}\leq Z\leq \frac{60-50}{5})=P(-2\leq Z\leq 2)=\phi(2)-\phi(-2)
\\P(400\leq X\leq 600)=P(\frac{400-500}{16}\leq Z\leq \frac{600-500}{16})=P(-6.25\leq Z\leq 6.25)=\phi(6.25)-\phi(-6.25)
$$

# è”åˆæ¦‚ç‡åˆ†å¸ƒéšæœºå˜é‡çš„ç‹¬ç«‹æ€§

$$
xä¸yç‹¬ç«‹å—ï¼Ÿ\\

\\ \color{yellow}{ä¾‹é¢˜1}\\ P_{x,y}=        \begin{cases}
            x+y,  & x,y\in[0,1] \\
            0, & other \\
        \end{cases}\\
        P_{x}=        \begin{cases}
            \int_0^1 (x+y)\cdot dy=x+\frac{1}{2} \\
            \int_0^1 0\cdot dy=0 \\
        \end{cases} \ \ \
          P_{y}=        \begin{cases}
            \int_0^1 (x+y)\cdot dx=y+\frac{1}{2} \\
            \int_0^1 0\cdot dx=0 \\
        \end{cases}\\
         P_{x,y}=        \begin{cases}
            (x+\frac{1}{2})\cdot (y+\frac{1}{2}) \\
            0 \\
        \end{cases}\neq \begin{cases}
            x+y \\
            0 \\
        \end{cases}\\ä¸ç‹¬ç«‹\\
\\ \color{yellow}{ä¾‹é¢˜2}\\ P_{x,y}=        \begin{cases}
            4xy,  & x,y\in[0,1] \\
            0, & other \\
        \end{cases}\\
        P_{x}=        \begin{cases}
            \int_0^1 4xy\cdot dy=2x \\
            \int_0^1 0\cdot dy=0 \\
        \end{cases} \ \ \
          P_{y}=        \begin{cases}
            \int_0^1 4xy\cdot dx=2y \\
            \int_0^1 0\cdot dx=0 \\
        \end{cases}\\
         P_{x,y}=        \begin{cases}
            2x\cdot 2y \\
            0 \\
        \end{cases}= \begin{cases}
            x+y \\
            0 \\
        \end{cases}\\ç‹¬ç«‹
$$

# æå¤§ä¼¼ç„¶ä¼°è®¡ä¸çŸ©ä¼°è®¡

$$
\color{red}æå¤§ä¼¼ç„¶ä¼°è®¡\\
å¯¹äºç¦»æ•£çš„X\backsim p(x;\theta),\{x_1,...,x_n\}æ˜¯ä¸€æ¬¡è§‚å¯Ÿå€¼,é‚£ä¹ˆäº‹ä»¶\{X_1=x_1,...,X_n=x_n \}å‘ç”Ÿçš„æ¦‚ç‡ä¸º\\L(\theta)=\prod_{i=1}^np(x;\theta)\\
ä¼¼ç„¶å‡½æ•°L(\theta)ã€æå¤§ç„¶ä¼°è®¡å€¼\hat\theta\{x_1,...,x_n\}ã€æå¤§ç„¶ä¼°è®¡é‡(MLE) \ \hat\theta\{X_1,...,X_n \}
\\ä¸ºäº†æ±‚L(\theta)æœ€å¤§å€¼å¯ä»¥æ±‚\ å¯¹æ•°ä¼¼ç„¶å‡½æ•°:lnL(\theta)æœ€å¤§
\\lnL(\theta)=\sum_{i=1}^{n} x_i\cdot lnp+(n-\sum_{i=1}^{n}x_i)\cdot ln(1-p)
\\\frac{\partial lnL(\theta)}{\partial\theta}=\frac{\sum x_i}{\theta}-\frac{n-\sum x_i}{1-\theta}\\
\hat\theta_{MLE}=\frac{1}{n}\sum x_i\\
\color{yellow}{ä¾‹é¢˜ï¼Œè‹¥X\backsim(\mu,\sigma^2),æ±‚\mu,\sigma^2çš„æå¤§ä¼¼ç„¶ä¼°è®¡}\\
\color{lime}step1 \
\color{silver}å·²çŸ¥P(x)=\frac{1}{\sqrt{2\pi}\cdot\sigma}\cdot e^{-\frac{(x-\mu)^2}{2\cdot\sigma^2}}\\ä»¤\mu=\theta_1,\sigma^2=\theta_2\implies P(x,\theta_1,\theta_2)=\frac{1}{\sqrt{2\pi}\cdot\theta_2^{\frac{1}{2}}}\cdot e^{-\frac{(x-\theta_1)^2}{2\cdot\theta_2}}\\
\color{lime}step2 \
\color{silver}L(\theta_1,\theta_2)=\prod_{i=1}^np(x,\theta_1,\theta_2)=(2\pi\theta_2)^{-\frac{n}{2}}\cdot e^{-\sum\frac{(x-\theta_1)^2}{2\theta_2}}\\
lnL(\theta_1,\theta_2)=-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\theta_2)-\sum\frac{(x-\theta_1)^2}{2\theta_2}\\
\color{lime}step3 \
\color{silver}\frac{\partial lnL}{\partial\theta_1}=0-0+\frac{1}{\theta_2}\sum (x_i-\theta_1)=\frac{1}{\theta_2}(\sum x_i-n\theta_1)=0\\\implies \theta_1=\frac{\sum x_i}{n}=\bar x\\
\frac{\partial lnL}{\partial\theta_2}=-\frac{n}{2\theta_2}+\sum\frac{(x-\theta_1)^2}{2\theta_2^2}=0\implies\theta_2=\frac{1}{n}\sum(x_i-\theta_1)^2=\frac{1}{n}\sum(x_i-\bar x)^2=S_2
\\\color{red}çŸ©ä¼°è®¡\\
å¸¸è§ç»Ÿè®¡é‡:\\
åŸç‚¹çŸ©M_k=E(x^k)=\frac{1}{n}\sum x_i^k\cdot P(X = x_i):\int[x^k \cdot f(x) dx]\\
æ ·æœ¬çŸ©(A_k): \frac{1}{n} \cdot \sum(x_i - A_1)^k\\
æ€»ä½“çš„ç´¯ç§¯é‡(B_k):\frac{1}{n}\cdot \sum\frac{i - 1} { n - 1} \cdot x_{i:n}\\
B_2=\frac{n-1}{n}S^2\\
\hat\sigma=\sqrt{A_2-{\bar X}^2}=\hat\sigma = \sqrt{\frac{1}{n-1} \sum (x_i - \bar X)^2}=\sqrt{B_2}\\

æ€»ä½“çŸ©(M_k^*,Î¼_k):E[(X - Î¼_1)^k]=E[(X - \bar x)^k]\\
æ€»ä½“çš„ä¸­å¿ƒçŸ©(v_k):E[(X - E(X))^k]\\
D(x)=Var(x)=v_2=E[(X - E(X))^2]\\
\color{yellow}{ä¾‹é¢˜ï¼Œ\begin{array}{c|lcr}
    x & \text{  5  } & \text{  11  } & \text{ 0 }\\
    \hline
    P & p_1 &p_2& 1-p_1-p_2 \\
\end{array},M_1=E(x)=\bar x,M_2=E(x^2)=\frac{1}{n}\sum x_i^2,æ±‚p_1,p_2}\\
M_1=E(x)=5p_1+11p_2=\bar x\ ,\ M_2=E(x^2)=25p_1+121p_2=\frac{1}{n}\sum x_i^2\\
ä¸¤å¼ä»£å…¥æ¶ˆå…ƒï¼Œ\begin{cases}
            \hat p_1=\frac{1}{30}(11\bar x -\frac{1}{n}\sum x_i^2) \\
             \hat p_2 =\frac{1}{66}(\frac{1}{n}\sum x_i^2-5\bar x)\\
        \end{cases}\\
\color{yellow}æœ‰æ•°æ®\{0,5,5,11\},åˆ™\bar x =\frac{21}{4},\frac{1}{4}\sum_{i=1}^{4}x_i^2=\frac{171}{4}\\
ä»£å…¥æ±‚å¾—\begin{cases}
            \hat p_1=\frac{1}{30}(\frac{231}{4}-\frac{171}{4})=\frac{1}{2} \\
             \hat p_2 =\frac{1}{66}(\frac{171}{4}-\frac{105}{4})=\frac{1}{4}\\
        \end{cases}\\
$$

# æœ€å°æ–¹å·®æ— åä¼°è®¡åŸç†

$$
\hat\theta_1=U_1(x_1,...,x_n)\backsim random,\hat\theta_2=U_2(x_1,...,x_n)
\\1)E(\hat\theta)=\theta \ ?
\\\hat\theta_1=\bar x,\hat\theta_2=medion=\bar x
\\2) \lim_{n \to \infty}D(\hat\theta)=0 \ ?
\\if \ D(\hat\theta_1)<D(\hat\theta_2)\implies \hat\theta_1\ is\ better
\\
\\ä¸€ä¸ªæ­£æ€åˆ†å¸ƒçš„æ€»ä½“X \backsim N(\mu,\sigma^2),å‡å€¼ä¸ºÎ¼,æ–¹å·®ä¸ºÏƒ^2
\\æˆ‘ä»¬ä»è¿™ä¸ªæ€»ä½“ä¸­éšæœºæŠ½å–äº†ä¸€ä¸ªå®¹é‡ä¸ºnçš„ç®€å•éšæœºæ ·æœ¬{x_1, x_2, ..., x_n}ã€‚\\ç°åœ¨æˆ‘ä»¬å¸Œæœ›ä¼°è®¡æ€»ä½“çš„å‡å€¼Î¼ã€‚æœ‰ä»¥ä¸‹ä¸¤ä¸ªä¼°è®¡é‡:
\\æ ·æœ¬å‡å€¼: \hatÎ¼_1 = \frac{1}{n} \sum_{i=1}^n x_i
\\
åˆ«çš„ä¼°è®¡é‡: \hatÎ¼_2 = \frac{1}{n+1} \sum_{i=1}^n x_i
\\
å“ªä¸ªä¼°è®¡é‡æ˜¯æ€»ä½“å‡å€¼Î¼çš„æœ€å°æ–¹å·®æ— åä¼°è®¡?\\
E(\hatÎ¼_1) = E(\frac{1}{n} \sum_{i=1}^n x_i) = \frac{1}{n} \sum_{i=1}^n E(x_i) = \frac{1}{n} \cdot nÎ¼ = Î¼\\


E(\hatÎ¼_2) = E(\frac{1}{n+1} \sum_{i=1}^n x_i) = \frac{1}{n+1} \sum_{i=1}^n E(x_i) = \frac{1}{n+1} \cdot nÎ¼ = \frac{n}{n+1}Î¼ â‰  Î¼\\
ä»ä¸Šé¢çš„è®¡ç®—å¯ä»¥çœ‹å‡º,Ë†Î¼_1æ˜¯æ€»ä½“å‡å€¼Î¼çš„æ— åä¼°è®¡,è€ŒË†Î¼_2ä¸æ˜¯æ— åä¼°è®¡ã€‚\\å› æ­¤,æˆ‘ä»¬ä¸éœ€è¦è®¡ç®—Ë†Î¼_2çš„æ–¹å·®,å› ä¸ºå®ƒå·²ç»ä¸æ»¡è¶³æ— åæ€§æ¡ä»¶ã€‚\\
D(\hat	Î¼_1) = D(\frac{1}{n} \sum_{i=1}^n x_i) = \frac{1}{n^2} \sum_{i=1}^n Var(x_i) = \frac{1}{n^2} \cdot nÏƒ^2 = \frac{Ïƒ^2}{n}\\æ‰€ä»¥,åœ¨è¿™ä¸ªä¾‹å­ä¸­,æ ·æœ¬å‡å€¼Ë†Î¼_1æ˜¯æ€»ä½“å‡å€¼Î¼çš„æœ€å°æ–¹å·®æ— åä¼°è®¡ã€‚\\å®ƒä¸ä»…æ»¡è¶³æ— åæ€§æ¡ä»¶,è€Œä¸”æ–¹å·®æœ€å°,ä¿è¯äº†ä¼°è®¡çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚
$$

# ä¹ é¢˜

## 1)

A diagnostic test for a certain disease is applied to n individuals known to not have the disease. Let X = the number among the n test results that are positive (indicating presence of the disease, so X is the number of false positives) and p 5 the probability that a disease-free individualâ€™s test result is positive (i.e., p is the true proportion of test results from disease-free individuals that are positive). Assume that only X is available rather than the actual sequence of test results.å¯¹nä¸ªå·²çŸ¥æ²¡æœ‰æ‚£ç—…çš„äººè¿›è¡ŒæŸç§ç–¾ç—…çš„è¯Šæ–­æµ‹è¯•ã€‚è®¾X = nä¸ªæ£€æµ‹ç»“æœä¸­é˜³æ€§çš„æ•°é‡ï¼ˆè¡¨ç¤ºå­˜åœ¨è¯¥ç–¾ç—…ï¼Œå› æ­¤Xä¸ºå‡é˜³æ€§çš„æ•°é‡ï¼‰ï¼Œp 5ä¸ºæ— ç—…ä¸ªä½“æ£€æµ‹ç»“æœä¸ºé˜³æ€§çš„æ¦‚ç‡ï¼ˆå³pä¸ºæ— ç—…ä¸ªä½“æ£€æµ‹ç»“æœä¸ºé˜³æ€§çš„çœŸå®æ¯”ä¾‹ï¼‰ã€‚å‡è®¾åªæœ‰Xå¯ç”¨ï¼Œè€Œä¸æ˜¯æµ‹è¯•ç»“æœçš„å®é™…åºåˆ—ã€‚

- Derive the maximum likelihood estimator of p. If n = 20 and x = 3, what is the estimate?æ¨å¯¼pçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡é‡ã€‚å¦‚æœn = 20, x = 3ï¼Œä¼°è®¡æ˜¯å¤šå°‘ï¼Ÿ
  $$
  L(p)=p^{\sum x_i}\cdot (1-p)^{n-\sum x_i}
  \\ \hat p_{MLE}=\frac{1}{n}\sum x_i =\bar x=\frac{3}{20}
  $$
  
- Is the estimator of part (a) unbiased?(a)éƒ¨åˆ†çš„ä¼°è®¡é‡æ˜¯å¦æ— åï¼Ÿ
  $$
  E(\hat p)=E(\frac{1}{n}\sum y_i )=\frac{1}{n}\sum E(y_i)=\frac{1}{n}\cdot n\cdot p=p 
  $$
  
- If n = 20 and x = 3, what is the mle of the probability (1 âˆ’ p)5  that none of the next five tests done on disease-free individuals are positive?å¦‚æœn = 20, x = 3ï¼Œå¯¹æ— ç—…ä¸ªä½“è¿›è¡Œçš„æ¥ä¸‹æ¥äº”æ¬¡æ£€æµ‹å‡ä¸ºé˜³æ€§çš„æ¦‚ç‡ï¼ˆ1 - pï¼‰5çš„å¹³æ–¹æ˜¯å¤šå°‘ï¼Ÿ
  $$
  \hat{(1-p)^5}=0\\
  f(\hat p)=f(p)\\
  {(1-p)^5}=(1-\frac{3}{20})^5
  $$
  

## 2)

Let X denote the proportion of allotted time that a randomly selected student spends working on a certain aptitude test. Suppose the pdf of X isè®¾Xè¡¨ç¤ºéšæœºé€‰æ‹©çš„å­¦ç”ŸèŠ±åœ¨æŸé¡¹èƒ½åŠ›å€¾å‘æµ‹è¯•ä¸Šçš„æ—¶é—´æ‰€å çš„æ¯”ä¾‹ã€‚å‡è®¾Xçš„pdfæ˜¯
$$
f (x , Î¸) = \begin{cases}
            (Î¸ + 1)x^Î¸,  &  0 â‰¤ x â‰¤ 1 \\
            0, & \text{otherwise} \\
        \end{cases}
$$
where Î¸ > âˆ’1. A random sample of ten students yields data x1 = 0.92, x2 = 0.79, x3 = 0.90, x4 = 0.65, x5 = 0.86, x6 = 0.47, x7 = 0.73, x8 = 0.97, x9 = 0.94, x10 = 0.77.  å¼ä¸­Î¸ >âˆ’1ã€‚éšæœºæŠ½æ ·10åå­¦ç”Ÿå¾—åˆ°çš„æ•°æ®x1 = 0.92, x2 = 0.79, x3 = 0.90, x4 = 0.65, x5 = 0.86, x6 = 0.47, x7 = 0.73, x8 = 0.97, x9 = 0.94, x10 = 0.77ã€‚

- Use the method of moments to obtain an estimator of Î¸, and then compute the estimate for this data.ç”¨çŸ©é‡æ³•å¾—åˆ°Î¸çš„ä¼°è®¡é‡ï¼Œç„¶åè®¡ç®—è¯¥æ•°æ®çš„ä¼°è®¡ã€‚
  $$
  E(x)=\int_{-\infin}^{+\infin}x\cdot f(x,\theta)dx=\int_{0}^{1}x(\theta + 1)x^\theta dx\\=\frac{\theta+1}{\theta+2}x^{\theta+2} \huge{|}\normalsize ^{1}_{0}=\frac{\theta+1}{\theta+2}=\bar x\implies\theta=\frac{1-2\bar x}{\bar x-1}\\
  \hat\theta_{MM}=\frac{1-2\bar x}{\bar x-1}
  $$
  
- Obtain the maximum likelihood estimator of Î¸, and then compute the estimate for the given data.å¾—åˆ°Î¸çš„æå¤§ä¼¼ç„¶ä¼°è®¡é‡ï¼Œç„¶åè®¡ç®—ç»™å®šæ•°æ®çš„ä¼°è®¡ã€‚
  $$
  L(\theta)=\prod_{i=1}^nf(x_i,\theta)=\prod_{i=1}^n(\theta+1)x_i^\theta=(\theta+1)^n(\prod_{i=1}^nx_i)^{\theta}\\
  lnL(\theta)=n\cdot ln(\theta+1)+\theta\sum_{i=1}^{n}lnx_i\\
  \frac{\partial lnL(\theta)}{\partial\theta}=n\cdot\frac{1}{\theta+1}+\sum_{i=1}^{n}lnx_i=0
  \\\implies\theta=\frac{\sum lnx_i}{n}-1\\
  \hat\theta_{MLE}=\frac{\sum lnx_i}{n}-1\\
  $$
  

# æ ·æœ¬æ–¹å·®çš„æœŸæœ›å€¼æ˜¯å¦ä¸ºæ€»ä½“æ–¹å·®çš„æ— åä¼°è®¡

$$
E(\hat\mu)=E(\frac{1}{n}\cdot n \cdot \mu)=\mu\\
E(\hat\sigma^2)=E(\frac{1}{n} \sum_{i=1}^n (x_i - \bar x)^2)=\frac{1}{n}\sum_{i=1}^{n}E((x_i - \bar x)^2)=\frac{1}{n}\sum_{i=1}^{n}D(x_i - \bar x)\\
è‹¥i=1æ—¶åˆ™æœ‰D(x_1 - \bar x)=D(x_i - \frac{\sum x}{n})=D(x_1-\frac{1}{n}x_1-\frac{1}{n}x_2-...-\frac{1}{n}x_n)\\=D(\frac{n-1}{n}x_1-...-\frac{1}{n}x_n)\\=(\frac{n-1}{n})^2\ D(x_1)+(-\frac{1}{n})^2\ D(x_2)+...+(-\frac{1}{n})^2\ D(x_n)\\=(\frac{n-1}{n})^2\ \sigma^2+ \overbrace{(-\frac{1}{n})^2\ \sigma^2+...+(-\frac{1}{n})^2\ \sigma^2}^{\text{n-1ä¸ª}} \\
=(\frac{(n-1)^2}{n^2}+\frac{n-1}{n^2})\sigma^2=\frac{(n-1)\cdot n}{n^2}\sigma^2=\frac{n-1}{n}\sigma^2\\
æ‰€ä»¥\frac{1}{n}\sum_{i=1}^{n}D(x_i - \bar x)=\frac{1}{n}\cdot n \cdot\frac{n-1}{n}\sigma^2=\frac{n-1}{n}\sigma^2\\
æ‰€ä»¥E(\hat\sigma^2)=E(\frac{1}{n} \sum_{i=1}^n (x_i - \bar x)^2)=\frac{n-1}{n}\sigma^2\\
å…¶ä¸­E(\frac{n}{n-1}\hat\sigma^2)=\overbrace{E(\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2)=E(S^2)}^{S^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2}=\sigma^2

\\\\
$$

# æ¢¯åº¦ä¸‹é™

[Bç«™]: https://www.bilibili.com/video/BV1oY411N7Xz/?spm_id_from=333.1387.homepage.video_card.click&amp;vd_source=31681a57655a676fa3a66526e9ea9676

## ä¸€å…ƒå‡½æ•°

$$
f(x)=x^2-3x+2\\
        \begin{cases}
           x_0=0 \\
            x_{n+1}=x_n-\xi f'(x_n) ,&\xi=0.2\\
        \end{cases}
        \\x_0=0,f(0)=2,f'(0)=-3
        \\x_1=0.6,f(0.6)=0.5,f'(0.6)=-1.8
$$

![image-20241201220413117](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201220413117.png)

## äºŒå…ƒå‡½æ•°

$$
f(x,y)=3x^2+5y^2+6xy+7x+1\\
df=(\frac{\partial f}{\partial x},\frac{\partial f}{\partial x})=(6x+6y+7,10y+6x)\\
\begin{cases}
(x_0,y_0)=(0,0)\\
df_0(0,0)=(7,0)\\
\end{cases}\ \
\begin{cases}
(x_1,y_0)=0-0.2(7,0)=(-1.4,0)\\
df_1(-1.4,0)=(-2.4,0)\\
\end{cases}
$$

![image-20241201222245908](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201222245908.png)

## çº¿æ€§å›å½’

$$
w_0+w_1x_i=\hat y_i--é¢„æµ‹å€¼
$$

## æŸå¤±å‡½æ•°

$$
MSE=\frac{1}{n}\sum(\hat y_i-y_i)^2=\frac{1}{n}\sum(w_0+w_1x_i-y_i)^2=F(w_o,w_1)=Aw_0^2+Bw_0w_1+Cw_1+...\\
df=(F'_{w_0},F'_{w_1})=(\frac{1}{n}\sum2(w_0+w_1x_i-y_i),\frac{1}{n}\sum2x_i(w_0+w_1x_i-y_i))
$$

# å¦‚ä½•åœ¨å¤§æ ·æœ¬ä¸­æ›´æ–°å›å½’

éšæœºæŠ½å–æ ·æœ¬å»å¯»æ‰¾df------éšæœºæ¢¯åº¦ä¸‹é™

![image-20241201225705262](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201225705262.png)

![image-20241201225742418](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201225742418.png)

# äºšå½“ç®—æ³•ï¼ˆåŠ¨é‡ï¼Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰

![image-20241201230004714](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201230004714.png)

![image-20241201230236294](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201230236294.png)

![image-20241201230418837](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201230418837.png)

![image-20241201230553106](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201230553106.png)

![image-20241201230734474](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241201230734474.png)

# ä¹ é¢˜

$$
1)Let \ X_1, X_2, Â· Â· Â· , X_n \ be \ a \ random\ sample\ from\ distribution\\
\begin{array}{c|lcr}
    x_i & \text{0} & \text{1}  \\
    \hline
    P & 1-p & p  \\
\end{array}\ \ \ using\ the\ Method\ of\ Moments\ find\ estimator\ of\ p.\\
After \ a\ series\ of\ n = 10\ trials\ this\ observations\ where\ made\\ 0, 1, 1, 0, 1, 1, 1, 0, 0, 1\ find\ the\ estimate\ of\ p.\\
        \begin{cases}
M_1=E(X)=\bar X=p\\
M_2=E(X^2)=\frac{1}{n}\sum X_i^2=p\\
        \end{cases}
        \\M_1-M_2\implies0=\bar X-\frac{1}{n}\sum X_i^2\\
        \hat p_{MM}=\bar X=\frac{1}{n}\sum X_i^2\\
        L(p)=p^{\sum X_i}\cdot(1-p)^{n-\sum X_i}\\
        \hat p_{MLE}=\frac{1}{n}\sum X_i=\bar X\\
        \because \{x_1,...,x_n \}=\{0, 1, 1, 0, 1, 1, 1, 0, 0, 1\}\\
        \therefore  \hat p_{MLE}=\frac{6}{10},\hat p_{MM}=\frac{6}{10}
$$

$$
2)Let X_1, X_2, Â· Â· Â· , X_n\  be\ a\ random\ sample\ from\ distribution\\
\begin{array}{c|lcr}
    x_i & \text{0} & \text{1} & \text{3} \\
    \hline
    P & 1-p_1-p_2 & p_1 & p_2 \\
\end{array}\ \text{using the Method of Moments find estimators of}\ p_1\  and\ p_2 \\
\text{After a series of n = 10 trials this observations where made 0, 1, 3, 0, 1, 1, 1, 3, 3, 1}\\\text{find the estimates of} \ p_1  \ and \ p_2.\text{Are the obtained estimators unbiased?}\\
        \begin{cases}
M_1=E(X)=\bar X=p_1+3p_2\\
\\M_2=E(X^2)=\frac{1}{n}\sum X_i^2=p_1+9p_2\\
        \end{cases}
        \begin{cases}
        M_2-M_1\implies p_2=\frac{1}{6}(\frac{1}{n}\sum X_i^2-\bar X)\\
        \\3M_1-M_2\implies p_1=\frac{1}{2}(3\bar X-\frac{1}{n}\sum X_i^2)\\
        \end{cases}\\
        \because \{x_1,...,x_n \}=\{0, 1, 3, 0, 1, 1, 1, 3, 3, 1\},\bar X=\frac{14}{10},\frac{1}{n}\sum X_i^2=\frac{32}{10}\\
        \therefore  \hat p_{1}=\frac{1}{2},\hat p_{2}=\frac{3}{10}\\
        S^2=\frac{1}{n-1}\cdot\sum (X_i-\bar X)^2=\frac{62}{45}\\
        E(X)=\frac{1}{2}+\frac{9}{10}=\frac{14}{10}=\bar X\\
        D(x)=\frac{1}{2}+\frac{27}{10}=\frac{16}{5}>S^2\\
        so,\hat \theta\ is\ better
$$

$$
3)\text{Consider a random sample$ X_1, X_2 , Â· Â· Â· , X_n $from the shifted exponential pdf}\\
   f(x)     \begin{cases}
\lambda e^{-\lambda(x-\theta)}&x\geq\theta\\
\\0&x<\theta\\
        \end{cases}\\
        \text{a.Obtain the maximum likelihood estimators of Î¸ and Î».}\\
\text{b. Obtain the method of moment estimators of Î¸ and Î».}\\
\text{c.  If n = 10 time observations are made,}\\
\text{resulting in the values 3.11, 0.64, 2.55, 2.20, 5.44, 3.42, 10.39, 8.93, 17.82, and1.30,}\\ \text{calculate the estimates of Î¸ and Î».}
\\
a) L(\theta,\lambda)=\prod_{i=1}^n\lambda e^{-\lambda(x_i-\theta)}=\lambda^n\prod_{i=1}^ne^{-\lambda(x_i-\theta)}\\
\ln L=n\ln\lambda+(-\lambda)\sum_{i=1}^{n}(x_i-\theta)=n\ln\lambda+\lambda n\theta-\lambda\sum_{i=1}^{n}(x_i)\\
\begin{cases}
\frac{\partial\ln L}{\partial \theta}=\lambda n >0 &\therefore \theta\uparrow,L(\theta)\uparrow\therefore \theta=X_{min}\ \implies \hat\theta_{MLE}=\{X_1,...,X_n\}_{min}\\ \\
\frac{\partial\ln L}{\partial \lambda}=\frac{n}{\lambda}+n\theta-\sum_{i=1}^{n}x_i=0&\therefore \lambda=\frac{n}{\sum_{i=1}^{n}x_i-n\theta}=\frac{1}{\bar x-\theta}\implies\hat\lambda_{MLE}=\frac{1}{\bar x-\hat\theta_{MLE}}
\end{cases}\\
b)\mu_1=E(x)
=\int_{-\infty}^{+\infty}xf(x)dx
=\int_{\theta}^{+\infty}x\lambda e^{-\lambda(x-\theta)}dx
=-xe^{-\lambda(x-\theta)}\large{|}_{\theta}^{+\infty}-\int_{\theta}^{+\infty}\lambda e^{-\lambda(x-\theta)}dx\\
=\lim_{x \to +\infty}xe^{-\lambda(x-\theta)}-(-\theta)+(-\frac{1}{\lambda}e^{-\lambda(x-\theta)}\large{|}_{\theta}^{+\infty})
=0+\theta+(\lim_{x \to +\infty}e^{-\lambda(x-\theta)}-\frac{1}{\lambda})\\
=\theta+\frac{1}{\lambda}\\
\mu_2=E(x^2)
=\int_{-\infty}^{+\infty}x^2f(x)dx
=\int_{\theta}^{+\infty}x^2\lambda e^{-\lambda(x-\theta)}dx\\
=-x^2 e^{-\lambda(x-\theta)}\large{|}_{\theta}^{+\infty}-\int_{\theta}^{+\infty}- e^{-\lambda(x-\theta)}d(x^2)\\
=\lim_{x \to +\infty}x^2e^{-\lambda(x-\theta)}-(-\theta^2)+2\int_{\theta}^{+\infty}x\lambda e^{-\lambda(x-\theta)}dx\\
=0+\theta^2+2(\theta+\frac{1}{\lambda})
=\theta^2+2\theta+\frac{2}{\lambda}\\
A_1=\bar x=\mu_1\implies\hat\theta_{MM}=\bar x-\frac{1}{\lambda}\\
A_2=\frac{1}{n}\sum x_i^2=\mu_2\implies\hat\lambda_{MM}=\frac{2}{\frac{1}{n}\sum x_i^2-\theta^2-2\theta}\\
c)\hat\theta_{MLE}=min(X)=0.64,\hat\lambda_{MLE}=\frac{1}{\bar x-\hat\theta_{MLE}}=\frac{1}{5.58-0.64}\approx0.2\\
\hat\theta_{MM}=\infty,\hat\lambda_{MM}=0
$$



# åŒºé—´ä¼°è®¡

åœ¨æŸäº›æƒ…å†µä¸‹,ä½¿ç”¨ç‚¹ä¼°è®¡å¯èƒ½ä¸å¤Ÿå‡†ç¡®æˆ–å¯é ,è€Œéœ€è¦ä½¿ç”¨åŒºé—´ä¼°è®¡ã€‚

è¿™æ˜¯å› ä¸ºç‚¹ä¼°è®¡ç»™å‡ºçš„æ˜¯ä¸€ä¸ªç¡®å®šçš„å€¼,è€Œä¸æ˜¯è€ƒè™‘åˆ°ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚

![image-20241207224719330](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20241207224719330.png)
$$
æˆ‘ä»¬çŸ¥é“ \psi \backsim U(-\frac{\pi}{2},\frac{\pi}{2})\\

è¿™æ„å‘³ç€\psiçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ä¸ºf_\psi(\psi)=\frac{1}{b-a} = \frac{1}{\pi}\quad \text{å¯¹äº } -\frac{\pi}{2} < \psi < \frac{\pi}{2}\\
æˆ‘ä»¬å®šä¹‰X = a + \tan(\psi),å…¶ä¸­ a  æ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚æˆ‘ä»¬æƒ³è¦æ‰¾åˆ° ( X ) çš„åˆ†å¸ƒ\\
a=?,è¿™æ—¶å¦‚æœä½¿ç”¨\bar\muæ˜¯ä¸å¯¹çš„\\
äºæ˜¯ X=a+tan\psi\to a=X-tan\psi\\
ç”±äº ( \psi ) æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—  P(\psi < \theta) ï¼š\\
F_\psi(\theta) = \frac{\theta }{\pi}, \quad \text{å¯¹äº } -\frac{\pi}{2} < \theta < \frac{\pi}{2}\\
å› æ­¤F_x(x)=P(X<x)=P(a+tan\psi<x)\\
=P(\psi<arctan(x-a))=F_\psi(arctan(x-a))=\frac{\arctan(x - a) }{\pi}\\
f_X(x)=\frac{d}{dx} F_X(x)=\frac{d}{dx} F_\psi(\arctan(x - a))\\
=F_\psi'(\arctan(x - a)) \cdot \frac{d}{dx} \arctan(x - a)=\frac{1}{\pi}[arctan(x-a)]'\\
=\frac{1}{\pi(1+(x-a)^2)}\backsim\frac{1}{x^2}(x\to \infty)\\
E(x)=\int_{-\infty}^{+\infty}x \dot \ \frac{1}{x^2}dx=\not \exists
$$

## ä¸¾ä¸ª[ğŸŒ°](https://tw.piliapp.com/emojis/chestnut/)

1ï¼‰ä½ çš„è½¦åäº†ï¼Œæ‰‹é‡Œåªæœ‰1kï¼Œä½†ä¿®è½¦è¦500-2kä¸ç­‰ï¼Œä¿®å¥½çš„æŠŠæ¡æœ‰å¤šå¤§

2ï¼‰å±±ä¸Šç§¯é›ªèåŒ–ä¼šå½¢æˆå±±æ´ª
$$
ç§¯é›ªé‡ï¼Œé£åŠ›ï¼Œå¡åº¦...\to æ•°æ®é›†\{x_1=10,x_2=10,...,x_n=24\}\to\\
\bar x\toæ”¿åºœé€šè¿‡z(æ°´åé«˜åº¦)=\bar x(å±±æ´ªé«˜åº¦)+5m\\
\to P(x<z)=0.99æ¥è®¾ç½®æ°´åé˜²æ­¢å±±æ´ª
$$

## ç½®ä¿¡åŒºé—´

### æ¢è½´é‡å’Œç»Ÿè®¡é‡çš„åŒºåˆ«

**æ¢è½´é‡**æ˜¯æ ·æœ¬å’Œå¾…ä¼°å‚æ•°çš„å‡½æ•°ï¼Œ å…¶åˆ†å¸ƒä¸ä¾èµ–äºä»»ä½•æœªçŸ¥å‚æ•°ï¼›

**ç»Ÿè®¡é‡**åªæ˜¯æ ·æœ¬çš„å‡½æ•°ï¼Œ å…¶åˆ†å¸ƒå¸¸ä¾èµ–äºæœªçŸ¥å‚æ•°.
$$
å·²çŸ¥æ€»ä½“X\backsim N(\mu,\sigma^2),\mu,\sigmaæœªçŸ¥ï¼Œè¦ä¼°è®¡\muã€‚è‹¥æ ·æœ¬ä¸ºX_1,X_2...X_n,ä»¥ä¸‹ä¸‰ä¸ªå“ªä¸ªæ˜¯æ¢è½´é‡å’Œç»Ÿè®¡é‡\\
\bar X,\frac{\bar X-\mu}{\sigma/\sqrt{n}},\frac{\bar X-\mu}{S/\sqrt{n}}\\
\bar X-ç»Ÿè®¡é‡\\
\frac{\bar X-\mu}{\sigma/\sqrt{n}}æœ‰ä¸¤ä¸ªæœªçŸ¥å‚æ•°\\
\frac{\bar X-\mu}{S/\sqrt{n}}-æ¢è½´é‡
$$

### å•ä¸ªæ­£æ€æ€»ä½“å‡å€¼çš„åŒºé—´ä¼°è®¡

$$
\text{$\sigma^2$å·²çŸ¥ $\implies \bar X \pm Z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n}$} \\

\text{$\sigma^2$æœªçŸ¥ $\implies \bar X \pm T_{\frac{\alpha}{2},n-1}\frac{S}{\sqrt n}$}\\

\text{$\mu,\sigma^2$æœªçŸ¥ $\implies \frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2},n-1}}<\sigma^2<\frac{(n-1)S^2}{\chi^2_{\frac{1-\alpha}{2},n-1}}$}\\
$$

### ä¸¤ä¸ªæ­£æ€æ€»ä½“å‚æ•°çš„åŒºé—´ä¼°è®¡

$$
\text{$\sigma_x^2,\sigma_y^2$å·²çŸ¥$\implies G=\frac{(\bar X-\bar Y)-(\mu_x-\mu_y)}{\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}}}\backsim N(0,1)$}\implies (\bar X-\bar Y)\pm Z_{\frac{\alpha}{2}}\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}}\\\\\\
\text{$\sigma_x^2=\sigma_y^2$æœªçŸ¥}\implies \hat\sigma^2=S_w^2=\frac{(n_x-1)S_x^2+(n_y-1)S_y^2}{n_x+n_y-2}\\\implies
\frac{(\bar X-\bar Y)-(\mu_x-\mu_y)}{S_w\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}}\backsim T_{n_x+n_y-2}\implies (\bar X-\bar Y)\pm T_{\frac{\alpha}{2},(n_x+n_y-2)}S_w\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}\\\\\\
\text{$\sigma_x^2\not=\sigma_y^2$æœªçŸ¥}\implies\begin{cases}
\frac{(\bar X-\bar Y)-(\mu_x-\mu_y)}{\sqrt{\frac{S_x^2}{n_x}+\frac{S_y^2}{n_y}}}\backsim N(0,1)\implies (\bar X-\bar Y)\pm Z_{\frac{\alpha}{2}}\sqrt{\frac{S_x^2}{n_x}+\frac{S_y^2}{n_y}}  & \text{if $n$ >30} \\
...\backsim T_k,k=min(n_x -1,n_y -1)\implies (\bar X-\bar Y)\pm T_{\frac{\alpha}{2},k}\sqrt{\frac{S_x^2}{n_x}+\frac{S_y^2}{n_y}}, & \text{if $n<30$} \\
\end{cases}
$$

# å‡è®¾éªŒè¯

### å•ä¸ªæ­£æ€æ€»ä½“å‡å€¼çš„å‡è®¾æ£€éªŒ

$$
åŒè¾¹å‡è®¾:\\
ç»Ÿè®¡é‡:Z_0=\frac{\bar X -\mu_0}{\sigma/\sqrt n},T_0=\frac{\bar X -\mu_0}{S/\sqrt n}\\
æ‹’ç»(å¤§äº†å°±æ‹’):|Z_0|\geq Z_{\frac\alpha2},|T_0|\geq T_{\frac\alpha2,n-1}\\
P\_(æ¯”\alphaå°äº†å°±æ‹’):P\_=2(1-\phi(|Z_0|)),P\_=2P\{T_{n-1}\geq|T_0|\}\\\\

å•è¾¹å‡è®¾:\\
æ‹’ç»:(Z_0\leq-Z_\alpha,Z_0\geq Z_\alpha),(T_0\leq-T_{\alpha,n-1},T_0\geq T_{\alpha,n-1})\\
P\_:(\phi(Z_0),1-\phi(Z_0)),(P\{T_{n-1}\leq T_0\},P\{T_{n-1}\geq T_0\})
$$

### ä¸¤ä¸ªæ­£æ€æ€»ä½“å‚æ•°çš„å‡è®¾æ£€éªŒ

$$
åŒè¾¹å‡è®¾:\\
\sigma_x\sigma_yå·²çŸ¥\impliesç»Ÿè®¡é‡:Z_0=\frac{\bar X -\bar Y}{\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}}},æ‹’ç»:|Z_0|\geq Z_{\frac\alpha2},P\_=2(1-\phi(|Z_0|))\\\\\\\\

\sigma_x=\sigma_yæœªçŸ¥\implies\hat\sigma^2=S_w^2=\frac{(n_x-1)S_x^2+(n_y-1)S_y^2}{n_x+n_y-2}\\\implies ç»Ÿè®¡é‡:T_0=\frac{\bar X -\bar Y}{S_w\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}}
æ‹’ç»:|T_0|\geq T_{\frac\alpha2,n_x+n_y-2},P\_=2P\{T_{n_x+n_y-2}\geq|T_0|\}\\\\\\\\

\sigma_x\not=\sigma_yæœªçŸ¥\implies T_0=\frac{\bar X -\bar Y}{\sqrt{\frac{S_x^2}{n_x}+\frac{S_y^2}{n_y}}}\implies \begin{cases}
æ‹’ç»:|T_0|\geq Z_{\frac\alpha2},P\_=2(1-\phi(|T_0|)) &n>30\\
æ‹’ç»:|T_0|\geq T_{\frac\alpha2,k},P\_=2P\{T_{k}\geq|T_0|\}&n<30\\
\end{cases}\\
k=min(n_x-1,n_y-1)||k=\frac{(\frac{S_x^2}{n_x}+\frac{S_y^2}{n_y})^2}{\frac{(\frac{S_x^2}{n_x})^2}{n_x-1}+\frac{(\frac{S_y^2}{n_y})^2}{n_y-1}}
$$

# æ‹Ÿåˆåº¦ä¼˜å…ˆæ£€éªŒ

$$
æ‹’ç»:Q_{n-1}=\sum_{i=1}^n\frac{(x_i-\hat x_i)^2}{\hat x_i}
=\sum_{i=1}^n\frac{(æ ·æœ¬å€¼-æœŸæœ›å€¼)^2}{æœŸæœ›å€¼}
\geq \chi^2_{\alpha,n-r-1}
$$







# Bootstrapï¼ˆè‡ªåŠ©æ³•ï¼‰

æ˜¯ä¸€ç§ç»Ÿè®¡å­¦ä¸­çš„é‡æŠ½æ ·æŠ€æœ¯ï¼Œä¸»è¦ç”¨äºä¼°è®¡æ ·æœ¬ç»Ÿè®¡é‡çš„åˆ†å¸ƒç‰¹å¾ã€‚å®ƒé€šè¿‡ä»åŸå§‹æ ·æœ¬ä¸­åå¤æŠ½å–è™šæ‹Ÿæ ·æœ¬æ¥è¿›è¡Œæ¨æ–­ï¼Œä»¥ä¸‹æ˜¯ Bootstrap çš„ä¸»è¦ç›®çš„å’Œåº”ç”¨ï¼š

### 1. ä¼°è®¡æ ‡å‡†è¯¯å·®

Bootstrap å¯ä»¥ç”¨æ¥ä¼°è®¡æ ·æœ¬ç»Ÿè®¡é‡ï¼ˆå¦‚å‡å€¼ã€æ–¹å·®ç­‰ï¼‰çš„æ ‡å‡†è¯¯å·®ã€‚é€šè¿‡ä»åŸå§‹æ ·æœ¬ä¸­è¿›è¡Œå¤šæ¬¡é‡æŠ½æ ·ï¼Œå¯ä»¥è®¡ç®—å‡ºæ¯æ¬¡æŠ½æ ·å¾—åˆ°çš„ç»Ÿè®¡é‡ï¼Œä»è€Œå¾—åˆ°å…¶åˆ†å¸ƒï¼Œè¿›è€Œä¼°è®¡æ ‡å‡†è¯¯å·®ã€‚

### 2. æ„å»ºç½®ä¿¡åŒºé—´

é€šè¿‡ Bootstrap æ–¹æ³•ï¼Œå¯ä»¥æ„å»ºæ ·æœ¬ç»Ÿè®¡é‡çš„ç½®ä¿¡åŒºé—´ã€‚å…·ä½“æ­¥éª¤æ˜¯ï¼š

- ä»åŸå§‹æ ·æœ¬ä¸­è¿›è¡Œå¤šæ¬¡é‡æŠ½æ ·ï¼ˆé€šå¸¸æ˜¯æ•°åƒæ¬¡ï¼‰ã€‚
- å¯¹æ¯ä¸ªé‡æŠ½æ ·è®¡ç®—æ‰€éœ€çš„ç»Ÿè®¡é‡ï¼ˆå¦‚å‡å€¼ï¼‰ã€‚
- æ ¹æ®è¿™äº›ç»Ÿè®¡é‡çš„åˆ†å¸ƒï¼Œç¡®å®šç½®ä¿¡åŒºé—´çš„ä¸Šä¸‹é™ã€‚

### 3. è¿›è¡Œå‡è®¾æ£€éªŒ

Bootstrap è¿˜å¯ä»¥ç”¨äºå‡è®¾æ£€éªŒã€‚é€šè¿‡æ¯”è¾ƒåŸå§‹æ ·æœ¬ç»Ÿè®¡é‡ä¸é‡æŠ½æ ·å¾—åˆ°çš„ç»Ÿè®¡é‡ï¼Œå¯ä»¥åˆ¤æ–­åŸå‡è®¾æ˜¯å¦æˆç«‹ã€‚

### 4. å¤„ç†å°æ ·æœ¬é—®é¢˜

åœ¨æ ·æœ¬é‡è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿçš„ç»Ÿè®¡æ–¹æ³•å¯èƒ½ä¸å¤Ÿç¨³å¥ã€‚Bootstrap æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤§é‡çš„è™šæ‹Ÿæ ·æœ¬ï¼Œå¯ä»¥æé«˜ä¼°è®¡çš„å¯é æ€§ã€‚

### 5. é€‚ç”¨äºå¤æ‚æ¨¡å‹

Bootstrap æ–¹æ³•ä¸ä¾èµ–äºæ•°æ®çš„åˆ†å¸ƒå‡è®¾ï¼Œå› æ­¤å¯ä»¥åº”ç”¨äºå„ç§å¤æ‚æ¨¡å‹å’Œéå‚æ•°ç»Ÿè®¡åˆ†æä¸­ã€‚

### å…·ä½“æ­¥éª¤

Bootstrap çš„åŸºæœ¬æ­¥éª¤å¦‚ä¸‹ï¼š

1. ä»åŸå§‹æ ·æœ¬ä¸­éšæœºæŠ½å–æ ·æœ¬ï¼Œå…è®¸é‡å¤æŠ½æ ·ï¼Œå½¢æˆä¸€ä¸ªè™šæ‹Ÿæ ·æœ¬ã€‚
2. è®¡ç®—è¯¥è™šæ‹Ÿæ ·æœ¬çš„ç»Ÿè®¡é‡ï¼ˆå¦‚å‡å€¼ï¼‰ã€‚
3. é‡å¤æ­¥éª¤ 1 å’Œ 2 å¤šæ¬¡ï¼ˆå¦‚ 1000 æ¬¡ï¼‰ï¼Œå¾—åˆ°ä¸€ç³»åˆ—ç»Ÿè®¡é‡ã€‚
4. æ ¹æ®è¿™äº›ç»Ÿè®¡é‡çš„åˆ†å¸ƒè¿›è¡Œåˆ†æï¼Œå¦‚è®¡ç®—æ ‡å‡†è¯¯å·®ã€æ„å»ºç½®ä¿¡åŒºé—´ç­‰ã€‚

# å¦‚ä½•è®¡ç®—äºŒç±»é”™è¯¯

$$
\beta=P(\frac{\mu_0-\mu_1}{\sigma/\sqrt{n}}-Z_{0.025}<Z<\frac{\mu_0-\mu_1}{\sigma/\sqrt{n}}+Z_{0.025})
$$

