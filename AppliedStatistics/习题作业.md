
$$
1)Let \ X_1, X_2, · · · , X_n \ be \ a \ random\ sample\ from\ distribution\\
\begin{array}{c|lcr}
    x_i & \text{0} & \text{1}  \\
    \hline
    P & 1-p & p  \\
\end{array}\ \ \ using\ the\ Method\ of\ Moments\ find\ estimator\ of\ p.\\
After \ a\ series\ of\ n = 10\ trials\ this\ observations\ where\ made\\ 0, 1, 1, 0, 1, 1, 1, 0, 0, 1\ find\ the\ estimate\ of\ p.\\
        \begin{cases}
M_1=E(X)=\bar X=p\\
M_2=E(X^2)=\frac{1}{n}\sum X_i^2=p\\
        \end{cases}
        \\M_1-M_2\implies0=\bar X-\frac{1}{n}\sum X_i^2\\
        \hat p_{MM}=\bar X=\frac{1}{n}\sum X_i^2\\
        L(p)=p^{\sum X_i}\cdot(1-p)^{n-\sum X_i}\\
        \hat p_{MLE}=\frac{1}{n}\sum X_i=\bar X\\
        \because \{x_1,...,x_n \}=\{0, 1, 1, 0, 1, 1, 1, 0, 0, 1\}\\
        \therefore  \hat p_{MLE}=\frac{6}{10},\hat p_{MM}=\frac{6}{10}
$$

$$
2)Let X_1, X_2, · · · , X_n\  be\ a\ random\ sample\ from\ distribution\\
\begin{array}{c|lcr}
    x_i & \text{0} & \text{1} & \text{3} \\
    \hline
    P & 1-p_1-p_2 & p_1 & p_2 \\
\end{array}\ \text{using the Method of Moments find estimators of}\ p_1\  and\ p_2 \\
\text{After a series of n = 10 trials this observations where made 0, 1, 3, 0, 1, 1, 1, 3, 3, 1}\\\text{find the estimates of} \ p_1  \ and \ p_2.\text{Are the obtained estimators unbiased?}\\
        \begin{cases}
M_1=E(X)=\bar X=p_1+3p_2\\
\\M_2=E(X^2)=\frac{1}{n}\sum X_i^2=p_1+9p_2\\
        \end{cases}
        \begin{cases}
        M_2-M_1\implies p_2=\frac{1}{6}(\frac{1}{n}\sum X_i^2-\bar X)\\
        \\3M_1-M_2\implies p_1=\frac{1}{2}(3\bar X-\frac{1}{n}\sum X_i^2)\\
        \end{cases}\\
        \because \{x_1,...,x_n \}=\{0, 1, 3, 0, 1, 1, 1, 3, 3, 1\},\bar X=\frac{14}{10},\frac{1}{n}\sum X_i^2=\frac{32}{10}\\
        \therefore  \hat p_{1}=\frac{1}{2},\hat p_{2}=\frac{3}{10}\\
        S^2=\frac{1}{n-1}\cdot\sum (X_i-\bar X)^2=\frac{62}{45}\\
        E(X)=\frac{1}{2}+\frac{9}{10}=\frac{14}{10}=\bar X\\
        D(x)=\frac{1}{2}+\frac{27}{10}=\frac{16}{5}>S^2\\
        so,\hat \theta\ is\ better
$$

$$
3)\text{Consider a random sample$ X_1, X_2 , · · · , X_n $from the shifted exponential pdf}\\
   f(x)     \begin{cases}
\lambda e^{-\lambda(x-\theta)}&x\geq\theta\\
\\0&x<\theta\\
        \end{cases}\\
        \text{a.Obtain the maximum likelihood estimators of θ and λ.}\\
\text{b. Obtain the method of moment estimators of θ and λ.}\\
\text{c.  If n = 10 time observations are made,}\\
\text{resulting in the values 3.11, 0.64, 2.55, 2.20, 5.44, 3.42, 10.39, 8.93, 17.82, and1.30,}\\ \text{calculate the estimates of θ and λ.}
\\
a) L(\theta,\lambda)=\prod_{i=1}^n\lambda e^{-\lambda(x_i-\theta)}=\lambda^n\prod_{i=1}^ne^{-\lambda(x_i-\theta)}\\
\ln L=n\ln\lambda+(-\lambda)\sum_{i=1}^{n}(x_i-\theta)=n\ln\lambda+\lambda n\theta-\lambda\sum_{i=1}^{n}(x_i)\\
\begin{cases}
\frac{\partial\ln L}{\partial \theta}=\lambda n >0 &\therefore \theta\uparrow,L(\theta)\uparrow\therefore \theta=X_{min}\ \implies \hat\theta_{MLE}=\{X_1,...,X_n\}_{min}\\ \\
\frac{\partial\ln L}{\partial \lambda}=\frac{n}{\lambda}+n\theta-\sum_{i=1}^{n}x_i=0&\therefore \lambda=\frac{n}{\sum_{i=1}^{n}x_i-n\theta}=\frac{1}{\bar x-\theta}\implies\hat\lambda_{MLE}=\frac{1}{\bar x-\hat\theta_{MLE}}
\end{cases}\\
b)\mu_1=E(x)
=\int_{-\infty}^{+\infty}xf(x)dx
=\int_{\theta}^{+\infty}x\lambda e^{-\lambda(x-\theta)}dx
=-xe^{-\lambda(x-\theta)}\large{|}_{\theta}^{+\infty}-\int_{\theta}^{+\infty}\lambda e^{-\lambda(x-\theta)}dx\\
=\lim_{x \to +\infty}xe^{-\lambda(x-\theta)}-(-\theta)+(-\frac{1}{\lambda}e^{-\lambda(x-\theta)}\large{|}_{\theta}^{+\infty})
=0+\theta+(\lim_{x \to +\infty}e^{-\lambda(x-\theta)}-\frac{1}{\lambda})\\
=\theta+\frac{1}{\lambda}\\
\mu_2=E(x^2)
=\int_{-\infty}^{+\infty}x^2f(x)dx
=\int_{\theta}^{+\infty}x^2\lambda e^{-\lambda(x-\theta)}dx\\
=-x^2 e^{-\lambda(x-\theta)}\large{|}_{\theta}^{+\infty}-\int_{\theta}^{+\infty}- e^{-\lambda(x-\theta)}d(x^2)\\
=\lim_{x \to +\infty}x^2e^{-\lambda(x-\theta)}-(-\theta^2)+2\int_{\theta}^{+\infty}x\lambda e^{-\lambda(x-\theta)}dx\\
=0+\theta^2+2(\theta+\frac{1}{\lambda})
=\theta^2+2\theta+\frac{2}{\lambda}\\
A_1=\bar x=\mu_1\implies\hat\theta_{MM}=\bar x-\frac{1}{\lambda}\\
A_2=\frac{1}{n}\sum x_i^2=\mu_2\implies\hat\lambda_{MM}=\frac{2}{\frac{1}{n}\sum x_i^2-\theta^2-2\theta}\\
c)\hat\theta_{MLE}=min(X)=0.64,\hat\lambda_{MLE}=\frac{1}{\bar x-\hat\theta_{MLE}}=\frac{1}{5.58-0.64}\approx0.2\\
\hat\theta_{MM}=\infty,\hat\lambda_{MM}=0
$$

